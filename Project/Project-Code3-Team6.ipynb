{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score \n",
    "from sklearn.metrics import make_scorer, mean_squared_error, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = pd.read_csv('newTrainingDataset.csv',sep=',',header=0,quotechar='\"')\n",
    "ValData = pd.read_csv('newValidationDataset.csv',sep=',',header=0,quotechar='\"')\n",
    "TestData = pd.read_csv('ProcessedTest.csv',sep=',',header=0,quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and Y variables\n",
    "Vars = list(TrainData)\n",
    "YTr = np.array(TrainData[Vars[0]])\n",
    "XTr = np.array(TrainData.loc[:,Vars[1:]])\n",
    "\n",
    "Vars = list(ValData)\n",
    "YVal = np.array(ValData[Vars[0]])\n",
    "XVal = np.array(ValData.loc[:,Vars[1:]])\n",
    "\n",
    "Vars = list(TestData)\n",
    "XTest = np.array(TestData.loc[:,Vars[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:44:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:56:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:02:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:12:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:16:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:20:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:25:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:31:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:50:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:53:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:03:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:07:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:10:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:13:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:17:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanj\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:20:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a6684a76a38c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     cv=5)\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYTr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# it turns out 'learning_rate' = 0.15 minimize log loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    913\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    225\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m    228\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1267\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1268\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1270\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tuning 'learning_rate'\n",
    "param_test2 = { \n",
    " 'learning_rate':[0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "}\n",
    "gsearch = GridSearchCV(\n",
    "    estimator = XGBClassifier(objective= 'binary:logistic',seed=27), \n",
    "    param_grid = param_test2, \n",
    "    scoring= 'neg_log_loss',\n",
    "    cv=5)\n",
    "gsearch.fit(XTr,YTr)\n",
    "gsearch.best_params_  \n",
    "# it turns out 'learning_rate' = 0.15 minimize log loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning 'max_depth'\n",
    "param_test = {\n",
    " 'max_depth':[1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "gsearch = GridSearchCV(\n",
    "    estimator = XGBClassifier(objective= 'binary:logistic', learning_rate=0.3 , seed=27), \n",
    "    param_grid = param_test, \n",
    "    scoring= 'neg_log_loss',\n",
    "    cv=5)\n",
    "gsearch.fit(XTr,YTr)\n",
    "gsearch.best_params_     \n",
    "# it turns out 'max_depth' = 5 minimize log loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning 'min_child_weight'\n",
    "param_test3 = {\n",
    " 'min_child_weight':[1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "gsearch3 = GridSearchCV(\n",
    "    estimator = XGBClassifier(objective= 'binary:logistic', learning_rate=0.3 , max_depth = 8, seed=27), \n",
    "    param_grid = param_test3, \n",
    "    scoring= 'neg_log_loss',\n",
    "    cv=5)\n",
    "gsearch3.fit(XTr,YTr)\n",
    "gsearch3.best_params_     \n",
    "# it turns out 'min_child_weight' = 4 minimize log loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\teval-logloss:0.57196\ttrain-logloss:0.57190\n",
      "[1]\teval-logloss:0.50797\ttrain-logloss:0.50762\n",
      "[2]\teval-logloss:0.47221\ttrain-logloss:0.47140\n",
      "[3]\teval-logloss:0.45094\ttrain-logloss:0.45054\n",
      "[4]\teval-logloss:0.43875\ttrain-logloss:0.43785\n",
      "[5]\teval-logloss:0.43179\ttrain-logloss:0.43079\n",
      "[6]\teval-logloss:0.42752\ttrain-logloss:0.42645\n",
      "[7]\teval-logloss:0.42494\ttrain-logloss:0.42403\n",
      "[8]\teval-logloss:0.42357\ttrain-logloss:0.42254\n",
      "[9]\teval-logloss:0.42275\ttrain-logloss:0.42162\n",
      "[10]\teval-logloss:0.42218\ttrain-logloss:0.42108\n",
      "[11]\teval-logloss:0.42191\ttrain-logloss:0.42083\n",
      "[12]\teval-logloss:0.42175\ttrain-logloss:0.42052\n",
      "[13]\teval-logloss:0.42156\ttrain-logloss:0.42034\n",
      "[14]\teval-logloss:0.42142\ttrain-logloss:0.42016\n",
      "[15]\teval-logloss:0.42137\ttrain-logloss:0.42011\n",
      "[16]\teval-logloss:0.42131\ttrain-logloss:0.41985\n",
      "[17]\teval-logloss:0.42136\ttrain-logloss:0.42000\n",
      "[18]\teval-logloss:0.42131\ttrain-logloss:0.41987\n",
      "[19]\teval-logloss:0.42133\ttrain-logloss:0.41987\n",
      "[20]\teval-logloss:0.42127\ttrain-logloss:0.41987\n",
      "[21]\teval-logloss:0.42125\ttrain-logloss:0.41974\n",
      "[22]\teval-logloss:0.42113\ttrain-logloss:0.41976\n",
      "[23]\teval-logloss:0.42115\ttrain-logloss:0.41972\n",
      "[24]\teval-logloss:0.42115\ttrain-logloss:0.41971\n",
      "[25]\teval-logloss:0.42113\ttrain-logloss:0.41960\n",
      "[26]\teval-logloss:0.42114\ttrain-logloss:0.41957\n",
      "[27]\teval-logloss:0.42111\ttrain-logloss:0.41948\n",
      "[28]\teval-logloss:0.42109\ttrain-logloss:0.41945\n",
      "[29]\teval-logloss:0.42110\ttrain-logloss:0.41942\n",
      "[30]\teval-logloss:0.42109\ttrain-logloss:0.41944\n",
      "[31]\teval-logloss:0.42110\ttrain-logloss:0.41932\n",
      "[32]\teval-logloss:0.42109\ttrain-logloss:0.41931\n",
      "[33]\teval-logloss:0.42109\ttrain-logloss:0.41923\n",
      "[34]\teval-logloss:0.42105\ttrain-logloss:0.41931\n",
      "[35]\teval-logloss:0.42106\ttrain-logloss:0.41932\n",
      "[36]\teval-logloss:0.42104\ttrain-logloss:0.41929\n",
      "[37]\teval-logloss:0.42105\ttrain-logloss:0.41929\n",
      "[38]\teval-logloss:0.42105\ttrain-logloss:0.41926\n",
      "[39]\teval-logloss:0.42106\ttrain-logloss:0.41924\n",
      "[40]\teval-logloss:0.42106\ttrain-logloss:0.41921\n",
      "[41]\teval-logloss:0.42105\ttrain-logloss:0.41924\n",
      "[42]\teval-logloss:0.42106\ttrain-logloss:0.41920\n",
      "[43]\teval-logloss:0.42105\ttrain-logloss:0.41919\n",
      "[44]\teval-logloss:0.42104\ttrain-logloss:0.41920\n",
      "[45]\teval-logloss:0.42103\ttrain-logloss:0.41919\n",
      "[46]\teval-logloss:0.42102\ttrain-logloss:0.41918\n",
      "[47]\teval-logloss:0.42103\ttrain-logloss:0.41918\n",
      "[48]\teval-logloss:0.42101\ttrain-logloss:0.41914\n",
      "[49]\teval-logloss:0.42099\ttrain-logloss:0.41908\n",
      "[50]\teval-logloss:0.42099\ttrain-logloss:0.41907\n",
      "[51]\teval-logloss:0.42100\ttrain-logloss:0.41909\n",
      "[52]\teval-logloss:0.42099\ttrain-logloss:0.41908\n",
      "[53]\teval-logloss:0.42098\ttrain-logloss:0.41908\n",
      "[54]\teval-logloss:0.42097\ttrain-logloss:0.41907\n",
      "[55]\teval-logloss:0.42097\ttrain-logloss:0.41908\n",
      "[56]\teval-logloss:0.42096\ttrain-logloss:0.41909\n",
      "[57]\teval-logloss:0.42095\ttrain-logloss:0.41908\n",
      "[58]\teval-logloss:0.42094\ttrain-logloss:0.41908\n",
      "[59]\teval-logloss:0.42095\ttrain-logloss:0.41906\n",
      "[60]\teval-logloss:0.42095\ttrain-logloss:0.41905\n",
      "[61]\teval-logloss:0.42096\ttrain-logloss:0.41907\n",
      "[62]\teval-logloss:0.42094\ttrain-logloss:0.41902\n",
      "[63]\teval-logloss:0.42095\ttrain-logloss:0.41900\n",
      "[64]\teval-logloss:0.42095\ttrain-logloss:0.41901\n",
      "[65]\teval-logloss:0.42093\ttrain-logloss:0.41900\n",
      "[66]\teval-logloss:0.42093\ttrain-logloss:0.41897\n",
      "[67]\teval-logloss:0.42093\ttrain-logloss:0.41897\n",
      "[68]\teval-logloss:0.42093\ttrain-logloss:0.41900\n",
      "[69]\teval-logloss:0.42093\ttrain-logloss:0.41896\n",
      "[70]\teval-logloss:0.42095\ttrain-logloss:0.41896\n",
      "[71]\teval-logloss:0.42095\ttrain-logloss:0.41895\n",
      "[72]\teval-logloss:0.42095\ttrain-logloss:0.41895\n",
      "[73]\teval-logloss:0.42096\ttrain-logloss:0.41893\n",
      "[74]\teval-logloss:0.42095\ttrain-logloss:0.41892\n",
      "[75]\teval-logloss:0.42092\ttrain-logloss:0.41896\n",
      "[76]\teval-logloss:0.42090\ttrain-logloss:0.41895\n",
      "[77]\teval-logloss:0.42090\ttrain-logloss:0.41894\n",
      "[78]\teval-logloss:0.42089\ttrain-logloss:0.41892\n",
      "[79]\teval-logloss:0.42092\ttrain-logloss:0.41894\n",
      "[80]\teval-logloss:0.42093\ttrain-logloss:0.41892\n",
      "[81]\teval-logloss:0.42093\ttrain-logloss:0.41891\n",
      "[82]\teval-logloss:0.42093\ttrain-logloss:0.41890\n",
      "[83]\teval-logloss:0.42094\ttrain-logloss:0.41890\n",
      "[84]\teval-logloss:0.42093\ttrain-logloss:0.41890\n",
      "[85]\teval-logloss:0.42089\ttrain-logloss:0.41888\n",
      "[86]\teval-logloss:0.42089\ttrain-logloss:0.41888\n",
      "[87]\teval-logloss:0.42089\ttrain-logloss:0.41888\n",
      "[88]\teval-logloss:0.42089\ttrain-logloss:0.41887\n",
      "[89]\teval-logloss:0.42090\ttrain-logloss:0.41889\n",
      "[90]\teval-logloss:0.42090\ttrain-logloss:0.41889\n",
      "[91]\teval-logloss:0.42092\ttrain-logloss:0.41887\n",
      "[92]\teval-logloss:0.42092\ttrain-logloss:0.41887\n",
      "[93]\teval-logloss:0.42092\ttrain-logloss:0.41887\n",
      "[94]\teval-logloss:0.42093\ttrain-logloss:0.41889\n",
      "[95]\teval-logloss:0.42093\ttrain-logloss:0.41886\n",
      "[96]\teval-logloss:0.42090\ttrain-logloss:0.41884\n",
      "[97]\teval-logloss:0.42090\ttrain-logloss:0.41884\n",
      "[98]\teval-logloss:0.42089\ttrain-logloss:0.41882\n",
      "[99]\teval-logloss:0.42089\ttrain-logloss:0.41883\n",
      "[100]\teval-logloss:0.42089\ttrain-logloss:0.41882\n",
      "[101]\teval-logloss:0.42092\ttrain-logloss:0.41884\n",
      "[102]\teval-logloss:0.42092\ttrain-logloss:0.41883\n",
      "[103]\teval-logloss:0.42092\ttrain-logloss:0.41884\n",
      "[104]\teval-logloss:0.42092\ttrain-logloss:0.41883\n",
      "[105]\teval-logloss:0.42092\ttrain-logloss:0.41886\n",
      "[106]\teval-logloss:0.42090\ttrain-logloss:0.41883\n",
      "[107]\teval-logloss:0.42085\ttrain-logloss:0.41880\n",
      "[108]\teval-logloss:0.42085\ttrain-logloss:0.41879\n",
      "[109]\teval-logloss:0.42085\ttrain-logloss:0.41881\n",
      "[110]\teval-logloss:0.42085\ttrain-logloss:0.41882\n",
      "[111]\teval-logloss:0.42086\ttrain-logloss:0.41882\n",
      "[112]\teval-logloss:0.42085\ttrain-logloss:0.41883\n",
      "[113]\teval-logloss:0.42085\ttrain-logloss:0.41882\n",
      "[114]\teval-logloss:0.42085\ttrain-logloss:0.41881\n",
      "[115]\teval-logloss:0.42086\ttrain-logloss:0.41882\n",
      "[116]\teval-logloss:0.42086\ttrain-logloss:0.41881\n",
      "[117]\teval-logloss:0.42086\ttrain-logloss:0.41880\n",
      "[118]\teval-logloss:0.42086\ttrain-logloss:0.41880\n",
      "[119]\teval-logloss:0.42086\ttrain-logloss:0.41879\n",
      "[120]\teval-logloss:0.42086\ttrain-logloss:0.41879\n",
      "[121]\teval-logloss:0.42087\ttrain-logloss:0.41879\n",
      "[122]\teval-logloss:0.42087\ttrain-logloss:0.41879\n",
      "[123]\teval-logloss:0.42087\ttrain-logloss:0.41879\n",
      "[124]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[125]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[126]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[127]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[128]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[129]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[130]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[131]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[132]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[133]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[134]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[135]\teval-logloss:0.42087\ttrain-logloss:0.41878\n",
      "[136]\teval-logloss:0.42087\ttrain-logloss:0.41877\n",
      "[137]\teval-logloss:0.42087\ttrain-logloss:0.41877\n",
      "[138]\teval-logloss:0.42087\ttrain-logloss:0.41876\n",
      "[139]\teval-logloss:0.42087\ttrain-logloss:0.41876\n",
      "[140]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[141]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[142]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[143]\teval-logloss:0.42086\ttrain-logloss:0.41878\n",
      "[144]\teval-logloss:0.42084\ttrain-logloss:0.41876\n",
      "[145]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[146]\teval-logloss:0.42085\ttrain-logloss:0.41875\n",
      "[147]\teval-logloss:0.42085\ttrain-logloss:0.41875\n",
      "[148]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[149]\teval-logloss:0.42084\ttrain-logloss:0.41876\n",
      "[150]\teval-logloss:0.42084\ttrain-logloss:0.41876\n",
      "[151]\teval-logloss:0.42084\ttrain-logloss:0.41875\n",
      "[152]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[153]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[154]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[155]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[156]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[157]\teval-logloss:0.42085\ttrain-logloss:0.41876\n",
      "[158]\teval-logloss:0.42085\ttrain-logloss:0.41875\n",
      "[159]\teval-logloss:0.42085\ttrain-logloss:0.41875\n",
      "[160]\teval-logloss:0.42086\ttrain-logloss:0.41876\n",
      "[161]\teval-logloss:0.42086\ttrain-logloss:0.41875\n",
      "[162]\teval-logloss:0.42086\ttrain-logloss:0.41873\n",
      "[163]\teval-logloss:0.42085\ttrain-logloss:0.41871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\teval-logloss:0.42090\ttrain-logloss:0.41874\n",
      "[165]\teval-logloss:0.42090\ttrain-logloss:0.41874\n",
      "[166]\teval-logloss:0.42091\ttrain-logloss:0.41874\n",
      "[167]\teval-logloss:0.42090\ttrain-logloss:0.41871\n",
      "[168]\teval-logloss:0.42090\ttrain-logloss:0.41868\n",
      "[169]\teval-logloss:0.42089\ttrain-logloss:0.41867\n",
      "[170]\teval-logloss:0.42090\ttrain-logloss:0.41867\n",
      "[171]\teval-logloss:0.42089\ttrain-logloss:0.41864\n",
      "[172]\teval-logloss:0.42088\ttrain-logloss:0.41864\n",
      "[173]\teval-logloss:0.42088\ttrain-logloss:0.41864\n",
      "[174]\teval-logloss:0.42085\ttrain-logloss:0.41864\n",
      "[175]\teval-logloss:0.42089\ttrain-logloss:0.41868\n",
      "[176]\teval-logloss:0.42088\ttrain-logloss:0.41868\n",
      "[177]\teval-logloss:0.42088\ttrain-logloss:0.41868\n",
      "[178]\teval-logloss:0.42089\ttrain-logloss:0.41868\n",
      "[179]\teval-logloss:0.42089\ttrain-logloss:0.41868\n",
      "[180]\teval-logloss:0.42090\ttrain-logloss:0.41868\n",
      "[181]\teval-logloss:0.42088\ttrain-logloss:0.41867\n",
      "[182]\teval-logloss:0.42087\ttrain-logloss:0.41868\n",
      "[183]\teval-logloss:0.42087\ttrain-logloss:0.41866\n",
      "[184]\teval-logloss:0.42088\ttrain-logloss:0.41865\n",
      "[185]\teval-logloss:0.42087\ttrain-logloss:0.41866\n",
      "[186]\teval-logloss:0.42085\ttrain-logloss:0.41864\n",
      "[187]\teval-logloss:0.42085\ttrain-logloss:0.41864\n",
      "[188]\teval-logloss:0.42084\ttrain-logloss:0.41864\n",
      "[189]\teval-logloss:0.42084\ttrain-logloss:0.41864\n",
      "[190]\teval-logloss:0.42085\ttrain-logloss:0.41864\n",
      "[191]\teval-logloss:0.42085\ttrain-logloss:0.41864\n",
      "[192]\teval-logloss:0.42088\ttrain-logloss:0.41865\n",
      "[193]\teval-logloss:0.42083\ttrain-logloss:0.41862\n",
      "[194]\teval-logloss:0.42083\ttrain-logloss:0.41860\n",
      "[195]\teval-logloss:0.42083\ttrain-logloss:0.41860\n",
      "[196]\teval-logloss:0.42084\ttrain-logloss:0.41860\n",
      "[197]\teval-logloss:0.42084\ttrain-logloss:0.41860\n",
      "[198]\teval-logloss:0.42084\ttrain-logloss:0.41861\n",
      "[199]\teval-logloss:0.42084\ttrain-logloss:0.41860\n",
      "[200]\teval-logloss:0.42084\ttrain-logloss:0.41859\n",
      "[201]\teval-logloss:0.42085\ttrain-logloss:0.41860\n",
      "[202]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[203]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[204]\teval-logloss:0.42087\ttrain-logloss:0.41860\n",
      "[205]\teval-logloss:0.42087\ttrain-logloss:0.41860\n",
      "[206]\teval-logloss:0.42085\ttrain-logloss:0.41857\n",
      "[207]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[208]\teval-logloss:0.42085\ttrain-logloss:0.41855\n",
      "[209]\teval-logloss:0.42084\ttrain-logloss:0.41860\n",
      "[210]\teval-logloss:0.42084\ttrain-logloss:0.41859\n",
      "[211]\teval-logloss:0.42084\ttrain-logloss:0.41859\n",
      "[212]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[213]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[214]\teval-logloss:0.42087\ttrain-logloss:0.41860\n",
      "[215]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[216]\teval-logloss:0.42087\ttrain-logloss:0.41861\n",
      "[217]\teval-logloss:0.42088\ttrain-logloss:0.41861\n",
      "[218]\teval-logloss:0.42088\ttrain-logloss:0.41861\n",
      "[219]\teval-logloss:0.42088\ttrain-logloss:0.41861\n",
      "[220]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[221]\teval-logloss:0.42087\ttrain-logloss:0.41860\n",
      "[222]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[223]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[224]\teval-logloss:0.42087\ttrain-logloss:0.41860\n",
      "[225]\teval-logloss:0.42088\ttrain-logloss:0.41857\n",
      "[226]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[227]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[228]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[229]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[230]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[231]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[232]\teval-logloss:0.42088\ttrain-logloss:0.41860\n",
      "[233]\teval-logloss:0.42088\ttrain-logloss:0.41859\n",
      "[234]\teval-logloss:0.42089\ttrain-logloss:0.41859\n",
      "[235]\teval-logloss:0.42090\ttrain-logloss:0.41859\n",
      "[236]\teval-logloss:0.42090\ttrain-logloss:0.41859\n",
      "[237]\teval-logloss:0.42090\ttrain-logloss:0.41859\n",
      "[238]\teval-logloss:0.42090\ttrain-logloss:0.41858\n",
      "[239]\teval-logloss:0.42089\ttrain-logloss:0.41858\n",
      "[240]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[241]\teval-logloss:0.42089\ttrain-logloss:0.41858\n",
      "[242]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[243]\teval-logloss:0.42088\ttrain-logloss:0.41858\n",
      "[244]\teval-logloss:0.42089\ttrain-logloss:0.41858\n",
      "[245]\teval-logloss:0.42088\ttrain-logloss:0.41857\n",
      "[246]\teval-logloss:0.42088\ttrain-logloss:0.41857\n",
      "[247]\teval-logloss:0.42088\ttrain-logloss:0.41857\n",
      "[248]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[249]\teval-logloss:0.42089\ttrain-logloss:0.41858\n",
      "[250]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[251]\teval-logloss:0.42089\ttrain-logloss:0.41858\n",
      "[252]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[253]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[254]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[255]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[256]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[257]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[258]\teval-logloss:0.42088\ttrain-logloss:0.41856\n",
      "[259]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[260]\teval-logloss:0.42089\ttrain-logloss:0.41856\n",
      "[261]\teval-logloss:0.42089\ttrain-logloss:0.41857\n",
      "[262]\teval-logloss:0.42089\ttrain-logloss:0.41856\n",
      "[263]\teval-logloss:0.42089\ttrain-logloss:0.41855\n",
      "[264]\teval-logloss:0.42083\ttrain-logloss:0.41850\n",
      "[265]\teval-logloss:0.42084\ttrain-logloss:0.41850\n",
      "[266]\teval-logloss:0.42083\ttrain-logloss:0.41850\n",
      "[267]\teval-logloss:0.42084\ttrain-logloss:0.41850\n",
      "[268]\teval-logloss:0.42087\ttrain-logloss:0.41852\n",
      "[269]\teval-logloss:0.42084\ttrain-logloss:0.41852\n",
      "[270]\teval-logloss:0.42084\ttrain-logloss:0.41852\n",
      "[271]\teval-logloss:0.42087\ttrain-logloss:0.41854\n",
      "[272]\teval-logloss:0.42087\ttrain-logloss:0.41854\n",
      "[273]\teval-logloss:0.42088\ttrain-logloss:0.41855\n",
      "[274]\teval-logloss:0.42088\ttrain-logloss:0.41856\n",
      "[275]\teval-logloss:0.42084\ttrain-logloss:0.41852\n",
      "[276]\teval-logloss:0.42085\ttrain-logloss:0.41851\n",
      "[277]\teval-logloss:0.42085\ttrain-logloss:0.41850\n",
      "[278]\teval-logloss:0.42085\ttrain-logloss:0.41852\n",
      "[279]\teval-logloss:0.42084\ttrain-logloss:0.41852\n",
      "[280]\teval-logloss:0.42085\ttrain-logloss:0.41852\n",
      "[281]\teval-logloss:0.42085\ttrain-logloss:0.41852\n",
      "[282]\teval-logloss:0.42085\ttrain-logloss:0.41852\n",
      "[283]\teval-logloss:0.42083\ttrain-logloss:0.41852\n",
      "[284]\teval-logloss:0.42084\ttrain-logloss:0.41851\n",
      "[285]\teval-logloss:0.42084\ttrain-logloss:0.41851\n",
      "[286]\teval-logloss:0.42084\ttrain-logloss:0.41852\n",
      "[287]\teval-logloss:0.42084\ttrain-logloss:0.41851\n",
      "[288]\teval-logloss:0.42083\ttrain-logloss:0.41851\n",
      "[289]\teval-logloss:0.42083\ttrain-logloss:0.41852\n",
      "[290]\teval-logloss:0.42083\ttrain-logloss:0.41852\n",
      "[291]\teval-logloss:0.42083\ttrain-logloss:0.41852\n",
      "[292]\teval-logloss:0.42083\ttrain-logloss:0.41852\n",
      "[293]\teval-logloss:0.42083\ttrain-logloss:0.41851\n",
      "[294]\teval-logloss:0.42083\ttrain-logloss:0.41851\n",
      "[295]\teval-logloss:0.42084\ttrain-logloss:0.41850\n",
      "[296]\teval-logloss:0.42083\ttrain-logloss:0.41849\n",
      "[297]\teval-logloss:0.42083\ttrain-logloss:0.41849\n",
      "[298]\teval-logloss:0.42084\ttrain-logloss:0.41849\n",
      "[299]\teval-logloss:0.42083\ttrain-logloss:0.41849\n",
      "[300]\teval-logloss:0.42083\ttrain-logloss:0.41849\n",
      "[301]\teval-logloss:0.42084\ttrain-logloss:0.41849\n",
      "[302]\teval-logloss:0.42084\ttrain-logloss:0.41849\n",
      "[303]\teval-logloss:0.42087\ttrain-logloss:0.41852\n",
      "[304]\teval-logloss:0.42088\ttrain-logloss:0.41851\n",
      "[305]\teval-logloss:0.42088\ttrain-logloss:0.41851\n",
      "[306]\teval-logloss:0.42088\ttrain-logloss:0.41850\n",
      "[307]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[308]\teval-logloss:0.42088\ttrain-logloss:0.41849\n",
      "[309]\teval-logloss:0.42089\ttrain-logloss:0.41850\n",
      "[310]\teval-logloss:0.42089\ttrain-logloss:0.41850\n",
      "[311]\teval-logloss:0.42089\ttrain-logloss:0.41850\n",
      "[312]\teval-logloss:0.42089\ttrain-logloss:0.41850\n",
      "[313]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[314]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[315]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[316]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[317]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[318]\teval-logloss:0.42090\ttrain-logloss:0.41849\n",
      "[319]\teval-logloss:0.42090\ttrain-logloss:0.41849\n",
      "[320]\teval-logloss:0.42090\ttrain-logloss:0.41849\n",
      "[321]\teval-logloss:0.42090\ttrain-logloss:0.41849\n",
      "[322]\teval-logloss:0.42089\ttrain-logloss:0.41848\n",
      "[323]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[324]\teval-logloss:0.42089\ttrain-logloss:0.41848\n",
      "[325]\teval-logloss:0.42089\ttrain-logloss:0.41848\n",
      "[326]\teval-logloss:0.42088\ttrain-logloss:0.41848\n",
      "[327]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[328]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[329]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[330]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[331]\teval-logloss:0.42088\ttrain-logloss:0.41847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[332]\teval-logloss:0.42088\ttrain-logloss:0.41849\n",
      "[333]\teval-logloss:0.42088\ttrain-logloss:0.41847\n",
      "[334]\teval-logloss:0.42088\ttrain-logloss:0.41848\n",
      "[335]\teval-logloss:0.42088\ttrain-logloss:0.41848\n",
      "[336]\teval-logloss:0.42088\ttrain-logloss:0.41849\n",
      "[337]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[338]\teval-logloss:0.42088\ttrain-logloss:0.41849\n",
      "[339]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[340]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[341]\teval-logloss:0.42089\ttrain-logloss:0.41849\n",
      "[342]\teval-logloss:0.42089\ttrain-logloss:0.41848\n",
      "[343]\teval-logloss:0.42088\ttrain-logloss:0.41846\n",
      "[344]\teval-logloss:0.42089\ttrain-logloss:0.41846\n",
      "[345]\teval-logloss:0.42088\ttrain-logloss:0.41846\n",
      "[346]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[347]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[348]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[349]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[350]\teval-logloss:0.42088\ttrain-logloss:0.41844\n",
      "[351]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[352]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[353]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[354]\teval-logloss:0.42088\ttrain-logloss:0.41844\n",
      "[355]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[356]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[357]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[358]\teval-logloss:0.42088\ttrain-logloss:0.41845\n",
      "[359]\teval-logloss:0.42089\ttrain-logloss:0.41846\n",
      "[360]\teval-logloss:0.42090\ttrain-logloss:0.41846\n",
      "[361]\teval-logloss:0.42089\ttrain-logloss:0.41846\n",
      "[362]\teval-logloss:0.42089\ttrain-logloss:0.41846\n",
      "[363]\teval-logloss:0.42090\ttrain-logloss:0.41846\n",
      "[364]\teval-logloss:0.42090\ttrain-logloss:0.41846\n",
      "[365]\teval-logloss:0.42089\ttrain-logloss:0.41846\n",
      "[366]\teval-logloss:0.42089\ttrain-logloss:0.41845\n",
      "[367]\teval-logloss:0.42090\ttrain-logloss:0.41845\n",
      "[368]\teval-logloss:0.42090\ttrain-logloss:0.41845\n",
      "[369]\teval-logloss:0.42089\ttrain-logloss:0.41845\n",
      "[370]\teval-logloss:0.42089\ttrain-logloss:0.41845\n",
      "[371]\teval-logloss:0.42089\ttrain-logloss:0.41845\n",
      "[372]\teval-logloss:0.42090\ttrain-logloss:0.41844\n",
      "[373]\teval-logloss:0.42089\ttrain-logloss:0.41844\n",
      "[374]\teval-logloss:0.42090\ttrain-logloss:0.41844\n",
      "[375]\teval-logloss:0.42090\ttrain-logloss:0.41845\n",
      "[376]\teval-logloss:0.42089\ttrain-logloss:0.41844\n",
      "[377]\teval-logloss:0.42089\ttrain-logloss:0.41844\n",
      "[378]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[379]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[380]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[381]\teval-logloss:0.42089\ttrain-logloss:0.41844\n",
      "[382]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[383]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[384]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[385]\teval-logloss:0.42089\ttrain-logloss:0.41843\n",
      "[386]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[387]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[388]\teval-logloss:0.42090\ttrain-logloss:0.41842\n",
      "[389]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[390]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[391]\teval-logloss:0.42090\ttrain-logloss:0.41844\n",
      "[392]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[393]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[394]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[395]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[396]\teval-logloss:0.42092\ttrain-logloss:0.41844\n",
      "[397]\teval-logloss:0.42091\ttrain-logloss:0.41844\n",
      "[398]\teval-logloss:0.42091\ttrain-logloss:0.41844\n",
      "[399]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[400]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[401]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[402]\teval-logloss:0.42090\ttrain-logloss:0.41843\n",
      "[403]\teval-logloss:0.42091\ttrain-logloss:0.41843\n",
      "[404]\teval-logloss:0.42091\ttrain-logloss:0.41842\n",
      "[405]\teval-logloss:0.42090\ttrain-logloss:0.41842\n",
      "[406]\teval-logloss:0.42091\ttrain-logloss:0.41842\n",
      "[407]\teval-logloss:0.42091\ttrain-logloss:0.41842\n",
      "[408]\teval-logloss:0.42090\ttrain-logloss:0.41842\n",
      "[409]\teval-logloss:0.42090\ttrain-logloss:0.41842\n",
      "[410]\teval-logloss:0.42091\ttrain-logloss:0.41842\n",
      "[411]\teval-logloss:0.42092\ttrain-logloss:0.41842\n",
      "[412]\teval-logloss:0.42091\ttrain-logloss:0.41841\n",
      "[413]\teval-logloss:0.42092\ttrain-logloss:0.41843\n",
      "[414]\teval-logloss:0.42091\ttrain-logloss:0.41841\n",
      "[415]\teval-logloss:0.42091\ttrain-logloss:0.41841\n",
      "[416]\teval-logloss:0.42091\ttrain-logloss:0.41841\n",
      "[417]\teval-logloss:0.42091\ttrain-logloss:0.41841\n",
      "[418]\teval-logloss:0.42092\ttrain-logloss:0.41841\n",
      "[419]\teval-logloss:0.42092\ttrain-logloss:0.41841\n",
      "[420]\teval-logloss:0.42092\ttrain-logloss:0.41840\n",
      "[421]\teval-logloss:0.42092\ttrain-logloss:0.41840\n",
      "[422]\teval-logloss:0.42092\ttrain-logloss:0.41840\n",
      "[423]\teval-logloss:0.42091\ttrain-logloss:0.41840\n",
      "[424]\teval-logloss:0.42091\ttrain-logloss:0.41840\n",
      "[425]\teval-logloss:0.42091\ttrain-logloss:0.41840\n",
      "[426]\teval-logloss:0.42091\ttrain-logloss:0.41840\n",
      "[427]\teval-logloss:0.42091\ttrain-logloss:0.41839\n",
      "[428]\teval-logloss:0.42091\ttrain-logloss:0.41839\n",
      "[429]\teval-logloss:0.42091\ttrain-logloss:0.41839\n",
      "[430]\teval-logloss:0.42091\ttrain-logloss:0.41839\n",
      "[431]\teval-logloss:0.42091\ttrain-logloss:0.41839\n",
      "[432]\teval-logloss:0.42093\ttrain-logloss:0.41839\n",
      "[433]\teval-logloss:0.42093\ttrain-logloss:0.41841\n",
      "[434]\teval-logloss:0.42093\ttrain-logloss:0.41840\n",
      "[435]\teval-logloss:0.42093\ttrain-logloss:0.41839\n",
      "[436]\teval-logloss:0.42092\ttrain-logloss:0.41838\n",
      "[437]\teval-logloss:0.42092\ttrain-logloss:0.41838\n",
      "[438]\teval-logloss:0.42092\ttrain-logloss:0.41838\n",
      "[439]\teval-logloss:0.42091\ttrain-logloss:0.41838\n",
      "[440]\teval-logloss:0.42091\ttrain-logloss:0.41838\n",
      "[441]\teval-logloss:0.42092\ttrain-logloss:0.41838\n",
      "[442]\teval-logloss:0.42091\ttrain-logloss:0.41837\n",
      "[443]\teval-logloss:0.42091\ttrain-logloss:0.41838\n",
      "[444]\teval-logloss:0.42091\ttrain-logloss:0.41838\n",
      "[445]\teval-logloss:0.42091\ttrain-logloss:0.41838\n",
      "[446]\teval-logloss:0.42092\ttrain-logloss:0.41838\n",
      "[447]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[448]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[449]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[450]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[451]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[452]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[453]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[454]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[455]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[456]\teval-logloss:0.42093\ttrain-logloss:0.41838\n",
      "[457]\teval-logloss:0.42093\ttrain-logloss:0.41838\n",
      "[458]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[459]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[460]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[461]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[462]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[463]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[464]\teval-logloss:0.42093\ttrain-logloss:0.41837\n",
      "[465]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[466]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[467]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[468]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[469]\teval-logloss:0.42093\ttrain-logloss:0.41835\n",
      "[470]\teval-logloss:0.42093\ttrain-logloss:0.41835\n",
      "[471]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[472]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[473]\teval-logloss:0.42092\ttrain-logloss:0.41835\n",
      "[474]\teval-logloss:0.42092\ttrain-logloss:0.41835\n",
      "[475]\teval-logloss:0.42092\ttrain-logloss:0.41834\n",
      "[476]\teval-logloss:0.42092\ttrain-logloss:0.41834\n",
      "[477]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[478]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[479]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[480]\teval-logloss:0.42092\ttrain-logloss:0.41835\n",
      "[481]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[482]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[483]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[484]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[485]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[486]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[487]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[488]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[489]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[490]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[491]\teval-logloss:0.42091\ttrain-logloss:0.41834\n",
      "[492]\teval-logloss:0.42091\ttrain-logloss:0.41835\n",
      "[493]\teval-logloss:0.42090\ttrain-logloss:0.41835\n",
      "[494]\teval-logloss:0.42090\ttrain-logloss:0.41834\n",
      "[495]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[496]\teval-logloss:0.42091\ttrain-logloss:0.41837\n",
      "[497]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[498]\teval-logloss:0.42094\ttrain-logloss:0.41838\n",
      "[499]\teval-logloss:0.42093\ttrain-logloss:0.41837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\teval-logloss:0.42095\ttrain-logloss:0.41837\n",
      "[501]\teval-logloss:0.42095\ttrain-logloss:0.41837\n",
      "[502]\teval-logloss:0.42095\ttrain-logloss:0.41837\n",
      "[503]\teval-logloss:0.42095\ttrain-logloss:0.41838\n",
      "[504]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[505]\teval-logloss:0.42092\ttrain-logloss:0.41836\n",
      "[506]\teval-logloss:0.42092\ttrain-logloss:0.41837\n",
      "[507]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[508]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[509]\teval-logloss:0.42093\ttrain-logloss:0.41836\n",
      "[510]\teval-logloss:0.42093\ttrain-logloss:0.41835\n",
      "[511]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[512]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[513]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[514]\teval-logloss:0.42094\ttrain-logloss:0.41835\n",
      "[515]\teval-logloss:0.42094\ttrain-logloss:0.41835\n",
      "[516]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[517]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[518]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[519]\teval-logloss:0.42094\ttrain-logloss:0.41836\n",
      "[520]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[521]\teval-logloss:0.42096\ttrain-logloss:0.41836\n",
      "[522]\teval-logloss:0.42095\ttrain-logloss:0.41836\n",
      "[523]\teval-logloss:0.42095\ttrain-logloss:0.41836\n",
      "[524]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[525]\teval-logloss:0.42096\ttrain-logloss:0.41836\n",
      "[526]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[527]\teval-logloss:0.42096\ttrain-logloss:0.41836\n",
      "[528]\teval-logloss:0.42095\ttrain-logloss:0.41836\n",
      "[529]\teval-logloss:0.42095\ttrain-logloss:0.41836\n",
      "[530]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[531]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[532]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[533]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[534]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[535]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[536]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[537]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[538]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[539]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[540]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[541]\teval-logloss:0.42095\ttrain-logloss:0.41835\n",
      "[542]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[543]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[544]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[545]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[546]\teval-logloss:0.42095\ttrain-logloss:0.41834\n",
      "[547]\teval-logloss:0.42092\ttrain-logloss:0.41830\n",
      "[548]\teval-logloss:0.42092\ttrain-logloss:0.41831\n",
      "[549]\teval-logloss:0.42092\ttrain-logloss:0.41831\n",
      "[550]\teval-logloss:0.42092\ttrain-logloss:0.41831\n",
      "[551]\teval-logloss:0.42092\ttrain-logloss:0.41831\n",
      "[552]\teval-logloss:0.42093\ttrain-logloss:0.41832\n",
      "[553]\teval-logloss:0.42093\ttrain-logloss:0.41833\n",
      "[554]\teval-logloss:0.42093\ttrain-logloss:0.41833\n",
      "[555]\teval-logloss:0.42093\ttrain-logloss:0.41832\n",
      "[556]\teval-logloss:0.42094\ttrain-logloss:0.41832\n",
      "[557]\teval-logloss:0.42094\ttrain-logloss:0.41833\n",
      "[558]\teval-logloss:0.42094\ttrain-logloss:0.41833\n",
      "[559]\teval-logloss:0.42094\ttrain-logloss:0.41832\n",
      "[560]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[561]\teval-logloss:0.42094\ttrain-logloss:0.41830\n",
      "[562]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[563]\teval-logloss:0.42093\ttrain-logloss:0.41829\n",
      "[564]\teval-logloss:0.42092\ttrain-logloss:0.41830\n",
      "[565]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[566]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[567]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[568]\teval-logloss:0.42096\ttrain-logloss:0.41831\n",
      "[569]\teval-logloss:0.42096\ttrain-logloss:0.41829\n",
      "[570]\teval-logloss:0.42097\ttrain-logloss:0.41829\n",
      "[571]\teval-logloss:0.42097\ttrain-logloss:0.41828\n",
      "[572]\teval-logloss:0.42097\ttrain-logloss:0.41829\n",
      "[573]\teval-logloss:0.42096\ttrain-logloss:0.41829\n",
      "[574]\teval-logloss:0.42096\ttrain-logloss:0.41829\n",
      "[575]\teval-logloss:0.42094\ttrain-logloss:0.41828\n",
      "[576]\teval-logloss:0.42094\ttrain-logloss:0.41828\n",
      "[577]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[578]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[579]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[580]\teval-logloss:0.42094\ttrain-logloss:0.41829\n",
      "[581]\teval-logloss:0.42095\ttrain-logloss:0.41829\n",
      "[582]\teval-logloss:0.42095\ttrain-logloss:0.41829\n",
      "[583]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[584]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[585]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[586]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[587]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[588]\teval-logloss:0.42095\ttrain-logloss:0.41830\n",
      "[589]\teval-logloss:0.42095\ttrain-logloss:0.41830\n",
      "[590]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[591]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[592]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[593]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[594]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[595]\teval-logloss:0.42094\ttrain-logloss:0.41831\n",
      "[596]\teval-logloss:0.42094\ttrain-logloss:0.41831\n",
      "[597]\teval-logloss:0.42094\ttrain-logloss:0.41831\n",
      "[598]\teval-logloss:0.42094\ttrain-logloss:0.41831\n",
      "[599]\teval-logloss:0.42094\ttrain-logloss:0.41831\n",
      "[600]\teval-logloss:0.42094\ttrain-logloss:0.41830\n",
      "[601]\teval-logloss:0.42094\ttrain-logloss:0.41830\n",
      "[602]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[603]\teval-logloss:0.42093\ttrain-logloss:0.41829\n",
      "[604]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[605]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[606]\teval-logloss:0.42092\ttrain-logloss:0.41830\n",
      "[607]\teval-logloss:0.42093\ttrain-logloss:0.41830\n",
      "[608]\teval-logloss:0.42093\ttrain-logloss:0.41832\n",
      "[609]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[610]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[611]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[612]\teval-logloss:0.42093\ttrain-logloss:0.41831\n",
      "[613]\teval-logloss:0.42093\ttrain-logloss:0.41829\n",
      "[614]\teval-logloss:0.42093\ttrain-logloss:0.41829\n",
      "[615]\teval-logloss:0.42093\ttrain-logloss:0.41829\n",
      "[616]\teval-logloss:0.42093\ttrain-logloss:0.41828\n",
      "[617]\teval-logloss:0.42093\ttrain-logloss:0.41828\n",
      "[618]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[619]\teval-logloss:0.42096\ttrain-logloss:0.41829\n",
      "[620]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[621]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[622]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[623]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[624]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[625]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[626]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[627]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[628]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[629]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[630]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[631]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[632]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[633]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[634]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[635]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[636]\teval-logloss:0.42095\ttrain-logloss:0.41828\n",
      "[637]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[638]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[639]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[640]\teval-logloss:0.42095\ttrain-logloss:0.41827\n",
      "[641]\teval-logloss:0.42095\ttrain-logloss:0.41827\n",
      "[642]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[643]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[644]\teval-logloss:0.42093\ttrain-logloss:0.41827\n",
      "[645]\teval-logloss:0.42093\ttrain-logloss:0.41827\n",
      "[646]\teval-logloss:0.42093\ttrain-logloss:0.41828\n",
      "[647]\teval-logloss:0.42092\ttrain-logloss:0.41827\n",
      "[648]\teval-logloss:0.42093\ttrain-logloss:0.41827\n",
      "[649]\teval-logloss:0.42093\ttrain-logloss:0.41827\n",
      "[650]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[651]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[652]\teval-logloss:0.42094\ttrain-logloss:0.41828\n",
      "[653]\teval-logloss:0.42094\ttrain-logloss:0.41828\n",
      "[654]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[655]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[656]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[657]\teval-logloss:0.42096\ttrain-logloss:0.41828\n",
      "[658]\teval-logloss:0.42095\ttrain-logloss:0.41827\n",
      "[659]\teval-logloss:0.42095\ttrain-logloss:0.41827\n",
      "[660]\teval-logloss:0.42095\ttrain-logloss:0.41827\n",
      "[661]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[662]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[663]\teval-logloss:0.42093\ttrain-logloss:0.41826\n",
      "[664]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[665]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[666]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[667]\teval-logloss:0.42094\ttrain-logloss:0.41826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[668]\teval-logloss:0.42093\ttrain-logloss:0.41826\n",
      "[669]\teval-logloss:0.42093\ttrain-logloss:0.41826\n",
      "[670]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[671]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[672]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[673]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[674]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[675]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[676]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[677]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[678]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[679]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[680]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[681]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[682]\teval-logloss:0.42094\ttrain-logloss:0.41827\n",
      "[683]\teval-logloss:0.42094\ttrain-logloss:0.41826\n",
      "[684]\teval-logloss:0.42098\ttrain-logloss:0.41829\n",
      "[685]\teval-logloss:0.42096\ttrain-logloss:0.41827\n",
      "[686]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[687]\teval-logloss:0.42096\ttrain-logloss:0.41827\n",
      "[688]\teval-logloss:0.42096\ttrain-logloss:0.41826\n",
      "[689]\teval-logloss:0.42096\ttrain-logloss:0.41826\n",
      "[690]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[691]\teval-logloss:0.42096\ttrain-logloss:0.41826\n",
      "[692]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[693]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[694]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[695]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[696]\teval-logloss:0.42097\ttrain-logloss:0.41828\n",
      "[697]\teval-logloss:0.42097\ttrain-logloss:0.41828\n",
      "[698]\teval-logloss:0.42096\ttrain-logloss:0.41828\n",
      "[699]\teval-logloss:0.42096\ttrain-logloss:0.41828\n",
      "[700]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[701]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[702]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[703]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[704]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[705]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[706]\teval-logloss:0.42096\ttrain-logloss:0.41827\n",
      "[707]\teval-logloss:0.42096\ttrain-logloss:0.41826\n",
      "[708]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[709]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[710]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[711]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[712]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[713]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[714]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[715]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[716]\teval-logloss:0.42097\ttrain-logloss:0.41827\n",
      "[717]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[718]\teval-logloss:0.42098\ttrain-logloss:0.41827\n",
      "[719]\teval-logloss:0.42098\ttrain-logloss:0.41827\n",
      "[720]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[721]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[722]\teval-logloss:0.42097\ttrain-logloss:0.41825\n",
      "[723]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[724]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[725]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[726]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[727]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[728]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[729]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[730]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[731]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[732]\teval-logloss:0.42097\ttrain-logloss:0.41826\n",
      "[733]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[734]\teval-logloss:0.42097\ttrain-logloss:0.41825\n",
      "[735]\teval-logloss:0.42097\ttrain-logloss:0.41824\n",
      "[736]\teval-logloss:0.42097\ttrain-logloss:0.41824\n",
      "[737]\teval-logloss:0.42096\ttrain-logloss:0.41824\n",
      "[738]\teval-logloss:0.42097\ttrain-logloss:0.41824\n",
      "[739]\teval-logloss:0.42097\ttrain-logloss:0.41824\n",
      "[740]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[741]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[742]\teval-logloss:0.42097\ttrain-logloss:0.41825\n",
      "[743]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[744]\teval-logloss:0.42097\ttrain-logloss:0.41823\n",
      "[745]\teval-logloss:0.42097\ttrain-logloss:0.41823\n",
      "[746]\teval-logloss:0.42098\ttrain-logloss:0.41823\n",
      "[747]\teval-logloss:0.42098\ttrain-logloss:0.41823\n",
      "[748]\teval-logloss:0.42098\ttrain-logloss:0.41823\n",
      "[749]\teval-logloss:0.42098\ttrain-logloss:0.41823\n",
      "[750]\teval-logloss:0.42099\ttrain-logloss:0.41823\n",
      "[751]\teval-logloss:0.42099\ttrain-logloss:0.41823\n",
      "[752]\teval-logloss:0.42098\ttrain-logloss:0.41823\n",
      "[753]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[754]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[755]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[756]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[757]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[758]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[759]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[760]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[761]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[762]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[763]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[764]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[765]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[766]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[767]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[768]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[769]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[770]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[771]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[772]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[773]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[774]\teval-logloss:0.42099\ttrain-logloss:0.41824\n",
      "[775]\teval-logloss:0.42099\ttrain-logloss:0.41824\n",
      "[776]\teval-logloss:0.42099\ttrain-logloss:0.41824\n",
      "[777]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[778]\teval-logloss:0.42098\ttrain-logloss:0.41824\n",
      "[779]\teval-logloss:0.42099\ttrain-logloss:0.41825\n",
      "[780]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[781]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[782]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[783]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[784]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[785]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[786]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[787]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[788]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[789]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[790]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[791]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[792]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[793]\teval-logloss:0.42098\ttrain-logloss:0.41826\n",
      "[794]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[795]\teval-logloss:0.42099\ttrain-logloss:0.41825\n",
      "[796]\teval-logloss:0.42099\ttrain-logloss:0.41825\n",
      "[797]\teval-logloss:0.42099\ttrain-logloss:0.41825\n",
      "[798]\teval-logloss:0.42098\ttrain-logloss:0.41825\n",
      "[799]\teval-logloss:0.42100\ttrain-logloss:0.41823\n",
      "[800]\teval-logloss:0.42100\ttrain-logloss:0.41823\n",
      "[801]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[802]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[803]\teval-logloss:0.42100\ttrain-logloss:0.41823\n",
      "[804]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[805]\teval-logloss:0.42100\ttrain-logloss:0.41823\n",
      "[806]\teval-logloss:0.42101\ttrain-logloss:0.41824\n",
      "[807]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[808]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[809]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[810]\teval-logloss:0.42101\ttrain-logloss:0.41824\n",
      "[811]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[812]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[813]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[814]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[815]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[816]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[817]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[818]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[819]\teval-logloss:0.42100\ttrain-logloss:0.41821\n",
      "[820]\teval-logloss:0.42100\ttrain-logloss:0.41821\n",
      "[821]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[822]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[823]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[824]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[825]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[826]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[827]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[828]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[829]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[830]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[831]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[832]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[833]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[834]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[835]\teval-logloss:0.42101\ttrain-logloss:0.41822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[836]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[837]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[838]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[839]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[840]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[841]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[842]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[843]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[844]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[845]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[846]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[847]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[848]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[849]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[850]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[851]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[852]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[853]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[854]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[855]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[856]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[857]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[858]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[859]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[860]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[861]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[862]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[863]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[864]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[865]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[866]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[867]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[868]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[869]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[870]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[871]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[872]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[873]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[874]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[875]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[876]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[877]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[878]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[879]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[880]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[881]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[882]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[883]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[884]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[885]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[886]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[887]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[888]\teval-logloss:0.42102\ttrain-logloss:0.41819\n",
      "[889]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[890]\teval-logloss:0.42102\ttrain-logloss:0.41819\n",
      "[891]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[892]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[893]\teval-logloss:0.42101\ttrain-logloss:0.41823\n",
      "[894]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[895]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[896]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[897]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[898]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[899]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[900]\teval-logloss:0.42101\ttrain-logloss:0.41821\n",
      "[901]\teval-logloss:0.42101\ttrain-logloss:0.41822\n",
      "[902]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[903]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[904]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[905]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[906]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[907]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[908]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[909]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[910]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[911]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[912]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[913]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[914]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[915]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[916]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[917]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[918]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[919]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[920]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[921]\teval-logloss:0.42103\ttrain-logloss:0.41820\n",
      "[922]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[923]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[924]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[925]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[926]\teval-logloss:0.42103\ttrain-logloss:0.41821\n",
      "[927]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[928]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[929]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[930]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[931]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[932]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[933]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[934]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[935]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[936]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[937]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[938]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[939]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[940]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[941]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[942]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[943]\teval-logloss:0.42102\ttrain-logloss:0.41822\n",
      "[944]\teval-logloss:0.42101\ttrain-logloss:0.41820\n",
      "[945]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[946]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[947]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[948]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[949]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[950]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[951]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[952]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[953]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[954]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[955]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[956]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[957]\teval-logloss:0.42102\ttrain-logloss:0.41821\n",
      "[958]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[959]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[960]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[961]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[962]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[963]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[964]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[965]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[966]\teval-logloss:0.42102\ttrain-logloss:0.41820\n",
      "[967]\teval-logloss:0.42102\ttrain-logloss:0.41819\n"
     ]
    }
   ],
   "source": [
    "#%% train model and make prediction \n",
    "dtrain = xgb.DMatrix(data=XTr,label=YTr)\n",
    "dval = xgb.DMatrix(data=XVal,label=YVal)\n",
    "\n",
    "param = { 'objective': 'binary:logistic', 'learning_rate':0.3, 'max_depth':8,'min_child_weight':7}\n",
    "num_round = 1500\n",
    "evallist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(param, dtrain, num_round, evallist,early_stopping_rounds=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on best round number \n",
    "ypredval = bst.predict(dval, ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42100175836175735"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.log_loss(YVal, ypredval, normalize=True, sample_weight=None, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test dataset clicks\n",
    "dtest = xgb.DMatrix(XTest)\n",
    "ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "# write out data set\n",
    "TestOutDF = pd.DataFrame(data={'YHatTest': ypred})\n",
    "TestOutDF.to_csv('TestYHatFrommnXGB.csv',sep=',',na_rep=\"NA\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
